use once_cell::sync::Lazy;
use regex::Regex;

/// List of bot patterns to detect crawlers - merged from multiple sources
const BOT_PATTERNS: &[&str] = &[
    "[ ]+bot",
    " daum[ /]",
    " deusu/",
    "newsbot",  // More specific pattern
    "googlebot",  // More specific pattern instead of lookahead
    " bot",  // More specific - requires space before bot
    "httpclient",  // More specific
    "scorecard",  // More specific
    "scanner",  // More specific
    "24x7",
    "@[a-z][\\w-]+\\.",
    "\\(\\)",
    "\\.com\\b",
    "\\btime/",
    "\\|",
    "^<",
    "^2ip.ru",
    "^[a-z.0-9/ \\-_]*bot",
    "^[^ ]{50,}$",
    "^\\d+\\b",
    "^\\w*search\\b",
    "^\\w+/[\\w\\(\\)]*$",
    "^active",
    "^ad muncher",
    "^amaya",
    "^avsdevicesdk/",
    "^azure",
    "^bidtellect/",
    "^biglotron",
    "^blackboard",
    "^blogtrottr",
    "^boardreader",
    "^bot",
    "^bw/",
    "^castro",
    "^clamav[ /]",
    "^client/",
    "^cobweb/",
    "^collectd",
    "^comodo",
    "^cortex",
    "^curl",
    "^custom",
    "^ddg[_-]android",
    "^ddg_android",
    "^discourse",
    "^dispatch/\\d",
    "^downcast/",
    "^duckduckgo",
    "^email",
    "^expanse",
    "^ez publish",
    "^facebook",
    "^fdm[\\s/]\\d",
    "^feedbin",
    "^fever",
    "^getright/",
    "^gozilla/",
    "^hobbit",
    "^holmes",
    "^hotzonu",
    "^hwcdn/",
    "^igetter/",
    "^java/",
    "^javascript",
    "^jeode/",
    "^jetty/",
    "^jigsaw",
    "^lcc ",
    "^lua-resty-http",
    "^microsoft bits",
    "^movabletype",
    "^mozilla/\\d\\.\\d\\s[\\w\\.-]+$",
    "^mozilla/\\d\\.\\d\\s\\(compatible;?(?:\\s\\w+\\/\\d+\\.\\d+)?\\)$",
    "^navermailapp",
    "^netlyzer fastprobe",
    "^netsurf",
    "^newsgator",
    "^ning/",
    "^octopus",
    "^offline",
    "^openai/",
    "^owler",
    "^pagepeeker",
    "^pagething",
    "^php",
    "^php-curl-class",
    "^postman",
    "^postmanruntime",
    "^prittorrent",
    "^python",
    "^rainmeter",
    "^ramblermail",
    "^rank",
    "^read",
    "^reed",
    "^rest",
    "^rss",
    "^sentry/",
    "^server density",
    "^sitesucker",
    "^snapchat",
    "^space bison",
    "^spotify/",
    "^sprinklr",
    "^svn",
    "^swcd ",
    "^taringa",
    "^the knowledge ai",
    "^thumbor/",
    "^track",
    "^unityplayer",
    "^viber$",
    "^w3c",
    "^webbandit/",
    "^webcopier",
    "^websitepulse",
    "^wget",
    "^whatsapp",
    "^whatsapp\\+?/[0-9\\.]+\\s[a-z]$",
    "^windows-rss",
    "^wordpress",
    "^wsr-agent",
    "^xenu link sleuth",
    "^yahoo",
    "^yahoo:linkexpander",
    "^yahoocachesystem",
    "^yandex",
    "^zdm/\\d",
    "^zoom marketplace/",
    "^zooshot",
    "a6-indexer",
    "aboundex",
    "adbeat",
    "addthis",
    "admantx",
    "adscanner",
    "agent",
    "ahc/",
    "aiohttp",
    "amazon cloudfront",
    "analyzer",
    "anyevent",
    "apachebench/",
    "apercite",
    "apis-google",
    "appengine-google",
    "appinsights",
    "arabot",
    "arachni",
    "archive",
    "archiver",
    "ask jeeves/teoma",
    "audit",
    "axios",
    "baidu-yunguance",
    "banca caboto",
    "barkrowler",
    "bazqux",
    "bit\\.ly/",
    "bluecoat drtr",
    "bot[/\\);-]",  // Simplified pattern
    "brandverity",
    "browsershots",
    "browsex",
    "btwebclient",
    "bubing",
    "buck/",
    "burpcollaborator",
    "capture",
    "catch",
    "catchpoint",
    "cc metadata scaper",
    "centuryb",
    "changedetection",
    "check\\b",
    "check_http",
    "checker",
    "checkmarknetwork/",
    "chrome-lighthouse",
    "chromeframe",
    "cincraw",
    "classifier",
    "clickagy",
    "cloudflare",
    "coccoc",
    "collection@infegy.com",
    "contextad bot",
    "convertify",
    "convera",
    "crawl",
    "crawler",
    "curious george",
    "cyberpatrol",
    "cypress/",
    "dareboost",
    "datadog agent",
    "datafeedwatch",
    "datanyze",
    "dataprovider.com",
    "daum(oa)?[ /][0-9]",
    "daum/",
    "dcrawl",
    "dejaclick",
    "detect",
    "digg deeper",
    "disqus",
    "dmbrowser",
    "domainreanimator",
    "domains project/",
    "download",
    "drupact",
    "duplexweb-google",
    "ec2linkfinder",
    "electricmonk",
    "embedly",
    "eright",
    "europarchive.org",
    "evc-batch/",
    "exaleadcloudview",
    "extractor",
    "ezid",
    "ezooms",
    "facebookexternalhit",
    "fedoraplanet",
    "feed",
    "feedly",
    "feedspot",
    "feedvalidator",
    "fetcher",
    "findlink",
    "findthatfile",
    "firephp",
    "flamingo_searchengine",
    "flipboardproxy",
    "fluffy",
    "freshrss",
    "friendica",
    "functionize",
    "g00g1e.net",
    "g2 web services",
    "genieo",
    "gigablast",
    "go-http-client",
    "gobuster",
    "gomezagent",
    "google favicon",
    "google page speed insight",
    "google search",
    "google web preview",
    "google-",
    "googleimageproxy",
    "goose/",
    "grab",
    "grouphigh/",
    "grub.org",
    "guzzlehttp",
    "gwene",
    "hatena",
    "headless",
    "headlesschrome",
    "help@dataminr\\.com",
    "heritrix",
    "httrack",
    "http[s]?://",
    "http_get",
    "httpclient",
    "httpunit",
    "httpurlconnection",
    "httpx",
    "hubspot",
    "hubspot marketing grader",
    "hydra",
    "ibisbrowser",
    "ichiro",
    "indeedbot",
    "infrawatch",
    "inoreader\\.com",
    "insight",
    "inspect",
    "integromedb",
    "internetarchive",
    "iplabel",
    "ips-agent",
    "iskanie",
    "java/",  // Simplified - match java/ instead
    "jetslide",
    "jetty",
    "kaspersky",
    "kouio\\.com",
    "larbin",
    "library",
    "libwww-perl",
    "linkcheck",
    "linkdex",
    "lipperhey",
    "livelapbot",
    "ltx71",
    "m_bot_tab",
    "mail\\.ru/",
    "manager",
    "mappydata",
    "mastodon",
    "measure",
    "mediapartners-google",
    "megaindex",
    "meltwaternews",
    "metauri",
    "miniflux/",
    "mixnodecache/",
    "mnogosearch",
    "moatbot",
    "monitoring",
    "moreover",
    "muckrack",
    "netcraft",
    "netresearchserver",
    "netsystemsresearch",
    "netvibes",
    "neustar wpm",
    "newsblur",
    "newsharecounts",
    "newspaper/",
    "nextcloud",
    "nmap scripting engine",
    "node",
    "node-fetch/",
    "nutch",
    "nuzzel",
    "offbyone",
    "okhttp",
    "omgili",
    "onetrust",
    "optimize",
    "optimizer",
    "outbrain",
    "pageburst",
    "page2rss",
    "pagepeeker/",
    "pagespeed",
    "pandalytics",
    "panscient",
    "parser",
    "pcore-http",
    "perl",
    "phantomjs",
    "phpcrawl",
    "pingdom",
    "pocketparser",
    "postrank",
    "powermarks",
    "pr-cy.ru",
    "preview",
    "proximic",
    "proxy",
    "prtg network monitor",
    "ptst[\\s/]",
    "pulsepoint",
    "pycurl",
    "python-requests",
    "python-urllib",
    "qihoobot",
    "qqdownload",
    "qwantify",
    "retriever",
    "rexx;",
    "rigor",
    "rivva",
    "robot",
    "robozilla",
    "rss\\b",
    "scrape",
    "scraper",
    "scrapy",
    "scoutjet",
    "searchatlas",
    "seewithkids",
    "seobility",
    "seokicks",
    "seolizer",
    "seoscanners",
    "server",
    "seznam",
    "simplepie",
    "site24x7",
    "siteexplorer.info",
    "siteimprove.com",
    "sixy\\.ch",
    "skypeuripreview",
    "slack-imgproxy",
    "slurp",
    "snacktory",
    "sogou",
    "sparkler/",
    "speedcurve",
    "spider",
    "splash",
    "sputnik",
    "statically-",
    "staticlogin:productcbox",
    "statuscake",
    "summify",
    "supercleaner",
    "superfeedr",
    "supybot",
    "swimgbot",
    "synapse",
    "sysomos",
    "synthetic",
    "teoma",
    "theoldreader.com",
    "thinkchaos",
    "thinklab",
    "tineye",
    "tiny tiny rss",
    "tools",
    "torrent",
    "traackr.com",
    "tracemyfile",
    "transcoder",
    "trendsmapresolver",
    "trove",
    "turbotabbee",
    "tweetedtimes",
    "twingly",
    "twurly",
    "um-ln",
    "unshortenit",
    "upflow",
    "uptime",
    "url",
    "urlgrabber/",
    "validator",
    "validator\\.nu",
    "vigil/",
    "virtuoso",
    "virustotal",
    "vkshare",
    "voilabot",
    "w3c-checklink",
    "w3c-mobileok",
    "w3c_css_validator",
    "w3c_unicorn",
    "w3c_validator",
    "wappalyzer",
    "webdatastats",
    "webglance",
    "webkit2png",
    "webmon ",
    "webreaper",
    "webthumbnail",
    "wesee:search",
    "whatcms/",
    "wordupinfosearch",
    "wotbox",
    "xtate/",
    "y!j",
    "yahoo link preview",
    "yak/",
    "yandexadnet",
    "yandexblogs",
    "yandexcalendar",
    "yandexdirect",
    "yandexfavicons",
    "yandexfordomain",
    "yandeximageresizer",
    "yandeximages",
    "yandexmarket",
    "yandexmedia",
    "yandexmetrika",
    "yandexnews",
    "yandexontodb",
    "yandexpartner",
    "yandexrca",
    "yandexsearchshop",
    "yandexsitelinks",
    "yandextracker",
    "yandexturbo",
    "yandexverticals",
    "yandexvertis",
    "yandexvideo",
    "yandexwebmaster",
    "yanga",
    "yeti",
    "zabbix",
    "zgrab",
    "temps",
];

/// Full compiled pattern combining all bot patterns
static FULL_PATTERN: Lazy<Regex> = Lazy::new(|| {
    let full_pattern_str = BOT_PATTERNS.join("|");
    Regex::new(&format!("(?i){}", full_pattern_str))
        .expect("Failed to compile full bot pattern")
});

/// Crawler detector that identifies bots and crawlers from user agent strings
pub struct CrawlerDetector;

impl CrawlerDetector {
    /// Check if the given user agent is a bot using the full pattern set
    pub fn is_bot(user_agent: Option<&str>) -> bool {
        match user_agent {
            Some(ua) if !ua.is_empty() => FULL_PATTERN.is_match(ua),
            _ => false,
        }
    }

    /// Find the first part of the user agent that matches a bot pattern
    pub fn find_bot_match(user_agent: Option<&str>) -> Option<String> {
        match user_agent {
            Some(ua) if !ua.is_empty() => {
                FULL_PATTERN.find(ua).map(|m| m.as_str().to_string())
            }
            _ => None,
        }
    }

    /// Get crawler name from user agent if it's a bot
    pub fn get_crawler_name(user_agent: Option<&str>) -> Option<String> {
        if Self::is_bot(user_agent) {
            Self::find_bot_match(user_agent)
        } else {
            None
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_common_bots() {
        // Test common bot user agents
        assert!(CrawlerDetector::is_bot(Some("Googlebot/2.1")));
        assert!(CrawlerDetector::is_bot(Some("Mozilla/5.0 (compatible; bingbot/2.0)")));
        assert!(CrawlerDetector::is_bot(Some("facebookexternalhit/1.1")));
        assert!(CrawlerDetector::is_bot(Some("Twitterbot/1.0")));
        assert!(CrawlerDetector::is_bot(Some("WhatsApp/2.19.81")));
        assert!(CrawlerDetector::is_bot(Some("Python-urllib/3.7")));
        assert!(CrawlerDetector::is_bot(Some("curl/7.64.1")));
        assert!(CrawlerDetector::is_bot(Some("Postman-Token/123")));
        assert!(CrawlerDetector::is_bot(Some("YandexBot/3.0")));
        assert!(CrawlerDetector::is_bot(Some("Slackbot-LinkExpanding 1.0")));
    }

    #[test]
    fn test_real_browsers() {
        // Test real browser user agents (should not be detected as bots)
        assert!(!CrawlerDetector::is_bot(Some(
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        )));
        assert!(!CrawlerDetector::is_bot(Some(
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/91.0.4472.124"
        )));
        assert!(!CrawlerDetector::is_bot(Some(
            "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15"
        )));
    }

    #[test]
    fn test_empty_user_agent() {
        assert!(!CrawlerDetector::is_bot(None));
        assert!(!CrawlerDetector::is_bot(Some("")));
    }

    #[test]
    fn test_crawler_name_extraction() {
        assert!(CrawlerDetector::get_crawler_name(Some("Googlebot/2.1")).is_some());
        assert!(CrawlerDetector::get_crawler_name(Some("Mozilla/5.0 (compatible; bingbot/2.0)")).is_some());
        assert_eq!(
            CrawlerDetector::get_crawler_name(Some("Regular Browser")),
            None
        );
    }
}
